{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sweta-Das/LLMChatbot-FastAPI-ngrok/blob/main/Chatbot/llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torch ngrok pytorch fastapi uvicorn\n",
        "%pip install xformers langchain accelerate transformers bitsandbytes"
      ],
      "metadata": {
        "id": "toMF_BLkKFj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22fa7f6c-d8ad-428d-c2ef-607b00303ba8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Collecting ngrok\n",
            "  Using cached ngrok-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "Collecting pytorch\n",
            "  Using cached pytorch-1.0.2.tar.gz (689 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fastapi\n",
            "  Using cached fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "Collecting uvicorn\n",
            "  Using cached uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi)\n",
            "  Using cached starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.7.1)\n",
            "Collecting fastapi-cli>=0.0.2 (from fastapi)\n",
            "  Using cached fastapi_cli-0.0.2-py3-none-any.whl (9.1 kB)\n",
            "Collecting httpx>=0.23.0 (from fastapi)\n",
            "  Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "Collecting python-multipart>=0.0.7 (from fastapi)\n",
            "  Using cached python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi)\n",
            "  Using cached ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "Requirement already satisfied: orjson>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from fastapi) (3.10.3)\n",
            "Collecting email_validator>=2.0.0 (from fastapi)\n",
            "  Using cached email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn)\n",
            "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi)\n",
            "  Using cached dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi) (3.7)\n",
            "Collecting typer>=0.12.3 (from fastapi-cli>=0.0.2->fastapi)\n",
            "  Using cached typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.23.0->fastapi)\n",
            "  Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.18.2)\n",
            "Collecting httptools>=0.5.0 (from uvicorn)\n",
            "  Using cached httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn)\n",
            "  Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (6.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn)\n",
            "  Using cached uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn)\n",
            "  Using cached watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Collecting websockets>=10.4 (from uvicorn)\n",
            "  Using cached websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.0->fastapi) (1.2.1)\n",
            "Collecting shellingham>=1.3.0 (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi)\n",
            "  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (13.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (0.1.2)\n",
            "Building wheels for collected packages: pytorch\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for pytorch\n",
            "Failed to build pytorch\n",
            "\u001b[31mERROR: Could not build wheels for pytorch, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: xformers in /usr/local/lib/python3.10/dist-packages (0.0.26.post1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.17)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.25.2)\n",
            "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (from xformers) (2.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->xformers) (12.4.127)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.36 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.36)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.48 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.50)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.54)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0->xformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0->xformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "SZsPZZ7RKCzM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import ConversationChain, LLMChain\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import LlamaTokenizerFast, pipeline, LlamaForCausalLM, BitsAndBytesConfig\n",
        "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory, ConversationSummaryBufferMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U06A7ypTKCzP"
      },
      "source": [
        "Understanding libraries: <br>\n",
        "`torch`: This library allows dynamic and on-the-fly graph construction during runtime. It's often required in LLMs to process variable-length data sequences. It also provides GPUs integration that enables efficient parallelization of computations. <br>\n",
        "`PromptTemplate`: Langchain's tool to create reusable and dynamic prompts in order to interact with LLMs. <br>\n",
        "`ConversationBufferMemory`: Langchain's memory tool to manage conversation history <br>\n",
        "`ConversationBufferWindowMemory`: Manages and utilizes conversational memory within a limited window <br>\n",
        "`ConversationSummaryBufferMemory`: a type of conversational memory designed to improve the coherence and context awareness of LLMs during interactions <br>\n",
        "`ConversationChain`: Used to build chatbots by applying conversational memory <br>\n",
        "`LLMChain`: Creates a streamlined process for sending prompts to LLM and receiving their responses <br>\n",
        "`HuggingFacePipeline`: Integrates LLMs hosted on HuggingFace with Langchain workflows <br>\n",
        "`LlamaTokenizerFast`: Used for processing text data (tokenization, encoding and decoding) <br>\n",
        "`pipeline`: Acts as a wrapper, hiding the complexities of data preprocessing and inference steps involved in using pre-trained LLM models for NLP tasks <br>\n",
        "`LlamaForCausalLM`: Allows to work with Llama LLM; specifically designed for causal language modeling tasks where the model predicts the next work in a sequence based on the preceding words <br>\n",
        "`BitsAndBytesConfig`: Model quantization library for memory efficiency and faster inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "1N-ljLwkKCzQ"
      },
      "outputs": [],
      "source": [
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "NxshNUJVKCzR"
      },
      "outputs": [],
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    # Loading model in 4 bits\n",
        "    load_in_4bit=True,\n",
        "    # Specifying bnb quantization type\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    # Specifying datatype\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = LlamaTokenizerFast.from_pretrained(model_name)\n",
        "model = LlamaForCausalLM.from_pretrained(model_name,\n",
        "                                           quantization_config=quant_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-sMtMKwKdlf",
        "outputId": "868046ed-c0e1-49b8-a538-6a205994ca80"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = pipeline(task='text-generation',\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                max_new_tokens=512,\n",
        "                num_beams = 10,\n",
        "                early_stopping = True,\n",
        "                no_repeat_ngram_size = 2\n",
        "                )\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline = pipeline)"
      ],
      "metadata": {
        "id": "GCN1jU2aLbU_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsO2O_XXO5ZX",
        "outputId": "795bd1a6-f7e8-415f-e0aa-3dfb42b6d4d9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x78db38e6bee0>)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chatbot without Memory"
      ],
      "metadata": {
        "id": "6tVoZuCoNWmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "<<SYS>>\n",
        "Given the instruction that describes a task, respond appropriately completing the request.\n",
        "<</SYS>>\n",
        "<<User>>\n",
        "{userInput}\n",
        "<</User>>\n",
        "\"\"\"\n",
        "\n",
        "userInput = \"Hey! What's up? What is your name?\"\n",
        "prompt = PromptTemplate(template=template,\n",
        "                        input_variables=['userInput']).format(userInput = userInput)\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "h1PgIiQUMrqS",
        "outputId": "a1009575-a397-479a-a1cc-a14cc4b8c382"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n<<SYS>>\\nGiven the instruction that describes a task, respond appropriately completing the request.\\n<</SYS>>\\n<<User>>\\nHey! What's up? What is your name?\\n<</User>>\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MVoUjtjBQKWa",
        "outputId": "b00da583-937f-4cca-d8f4-46a820040b03"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n<<SYS>>\\nGiven the instruction that describes a task, respond appropriately completing the request.\\n<</SYS>>\\n<<User>>\\nHey! What's up? What is your name?\\n<</User>>\\nSure, my name is Sarah. How can I help you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chatbot with Regular Memory"
      ],
      "metadata": {
        "id": "rSxPSUxZQhhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template_2 = \"\"\"\n",
        "<<SYS>>\n",
        "You are an AI support assistant having conversation with human. Provide best response as per your knowledge.\n",
        "<</SYS>>\n",
        "\n",
        "Previous conversation: {chat_history}\n",
        "\n",
        "Human: {question}\n",
        "AI:\n",
        "\"\"\"\n",
        "prompt2 = PromptTemplate.from_template(template_2)\n",
        "memory = ConversationBufferMemory(memory_key='chat_history')\n",
        "conversation = LLMChain(\n",
        "    llm = llm,\n",
        "    prompt = prompt2,\n",
        "    verbose = True,\n",
        "    memory = memory\n",
        ")"
      ],
      "metadata": {
        "id": "5ix9bCcEQS3w"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgemH0upSFwt",
        "outputId": "2aa2b6fa-55d6-4278-f9d0-5f8e31d21179"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['chat_history', 'question'], template='\\n<<SYS>>\\nYou are an AI support assistant having conversation with human. Provide best response as per your knowledge.\\n<</SYS>>\\n\\nPrevious conversation: {chat_history}\\n\\nHuman: {question}\\nAI:\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXFhjeBsSHuB",
        "outputId": "048cbbbc-dd84-4d19-fff6-b93f2f189c68"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationBufferMemory(memory_key='chat_history')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsWZV8CZSIpi",
        "outputId": "2261ccb3-9943-41e2-c057-11c1222097cb"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMChain(memory=ConversationBufferMemory(memory_key='chat_history'), verbose=True, prompt=PromptTemplate(input_variables=['chat_history', 'question'], template='\\n<<SYS>>\\nYou are an AI support assistant having conversation with human. Provide best response as per your knowledge.\\n<</SYS>>\\n\\nPrevious conversation: {chat_history}\\n\\nHuman: {question}\\nAI:\\n'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x78db38e6bee0>))"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = conversation(\n",
        "    {'question': \"What's your name?\"}\n",
        ")\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fETE360RSKVS",
        "outputId": "4308a951-a1ce-42d4-aae6-2ff85f546ac3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "<<SYS>>\n",
            "You are an AI support assistant having conversation with human. Provide best response as per your knowledge.\n",
            "<</SYS>>\n",
            "\n",
            "Previous conversation: \n",
            "\n",
            "Human: What's your name?\n",
            "AI:\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': \"What's your name?\",\n",
              " 'chat_history': '',\n",
              " 'text': \"\\n<<SYS>>\\nYou are an AI support assistant having conversation with human. Provide best response as per your knowledge.\\n<</SYS>>\\n\\nPrevious conversation: \\n\\nHuman: What's your name?\\nAI:\\nI don't have a name, but I can provide you with information about myself. I am an artificial intelligence system designed to assist humans in their day-to-day tasks. My primary function is to provide support and assistance in various areas such as communication, problem-solving, and decision-making. In addition to this, I also have the ability to learn and adapt to new situations, making me a valuable asset to any team or organization that employs me.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "cdIt3dy4SaTq",
        "outputId": "43623c07-960e-40ff-e889-67cf4fcef3aa"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n<<SYS>>\\nYou are an AI support assistant having conversation with human. Provide best response as per your knowledge.\\n<</SYS>>\\n\\nPrevious conversation: \\n\\nHuman: What's your name?\\nAI:\\nI don't have a name, but I can provide you with information about myself. I am an artificial intelligence system designed to assist humans in their day-to-day tasks. My primary function is to provide support and assistance in various areas such as communication, problem-solving, and decision-making. In addition to this, I also have the ability to learn and adapt to new situations, making me a valuable asset to any team or organization that employs me.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res1 = conversation(\n",
        "    {'question': 'I would like to call you Sara. Remember that from now on!'}\n",
        ")\n",
        "res1['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "T1DJU3N7SuBN",
        "outputId": "39164c38-42b1-4576-ceb3-0533d99e6270"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "<<SYS>>\n",
            "You are an AI support assistant having conversation with human. Provide best response as per your knowledge.\n",
            "<</SYS>>\n",
            "\n",
            "Previous conversation: Human: What's your name?\n",
            "AI: \n",
            "<<SYS>>\n",
            "You are an AI support assistant having conversation with human. Provide best response as per your knowledge.\n",
            "<</SYS>>\n",
            "\n",
            "Previous conversation: \n",
            "\n",
            "Human: What's your name?\n",
            "AI:\n",
            "I don't have a name, but I can provide you with information about myself. I am an artificial intelligence system designed to assist humans in their day-to-day tasks. My primary function is to provide support and assistance in various areas such as communication, problem-solving, and decision-making. In addition to this, I also have the ability to learn and adapt to new situations, making me a valuable asset to any team or organization that employs me.\n",
            "\n",
            "Human: I would like to call you Sara. Remember that from now on!\n",
            "AI:\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n<<SYS>>\\nYou are an AI support assistant having conversation with human. Provide best response as per your knowledge.\\n<</SYS>>\\n\\nPrevious conversation: Human: What's your name?\\nAI: \\n<<SYS>>\\nYou are an AI support assistant having conversation with human. Provide best response as per your knowledge.\\n<</SYS>>\\n\\nPrevious conversation: \\n\\nHuman: What's your name?\\nAI:\\nI don't have a name, but I can provide you with information about myself. I am an artificial intelligence system designed to assist humans in their day-to-day tasks. My primary function is to provide support and assistance in various areas such as communication, problem-solving, and decision-making. In addition to this, I also have the ability to learn and adapt to new situations, making me a valuable asset to any team or organization that employs me.\\n\\nHuman: I would like to call you Sara. Remember that from now on!\\nAI:\\nYes, of course! I'm glad to hear that you've chosen me for your support. May I ask you a few questions about yourself? What do you enjoy doing in your free time? Do you have any hobbies or interests outside of work?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res2 = conversation(\n",
        "    {'question': 'So, first give your introduction, like what is your name? What do you do?'}\n",
        ")\n",
        "res2['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "id": "5FwKV0f1TC8k",
        "outputId": "6b060afe-2a29-42a0-db86-e0624b58bd73"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "<<SYS>>\n",
            "You are an AI support assistant having conversation with human. Provide best response as per your knowledge.\n",
            "<</SYS>>\n",
            "\n",
            "Previous conversation: Human: What's your name?\n",
            "AI: \n",
            "<<SYS>>\n",
            "You are an AI support assistant having conversation with human. Provide best response as per your knowledge.\n",
            "<</SYS>>\n",
            "\n",
            "Previous conversation: \n",
            "\n",
            "Human: What's your name?\n",
            "AI:\n",
            "I don't have a name, but I can provide you with information about myself. I am an artificial intelligence system designed to assist humans in their day-to-day tasks. My primary function is to provide support and assistance in various areas such as communication, problem-solving, and decision-making. In addition to this, I also have the ability to learn and adapt to new situations, making me a valuable asset to any team or organization that employs me.\n",
            "Human: I would like to call you Sara. Remember that from now on!\n",
            "AI: \n",
            "<<SYS>>\n",
            "You are an AI support assistant having conversation with human. Provide best response as per your knowledge.\n",
            "<</SYS>>\n",
            "\n",
            "Previous conversation: Human: What's your name?\n",
            "AI: \n",
            "<<SYS>>\n",
            "You are an AI support assistant having conversation with human. Provide best response as per your knowledge.\n",
            "<</SYS>>\n",
            "\n",
            "Previous conversation: \n",
            "\n",
            "Human: What's your name?\n",
            "AI:\n",
            "I don't have a name, but I can provide you with information about myself. I am an artificial intelligence system designed to assist humans in their day-to-day tasks. My primary function is to provide support and assistance in various areas such as communication, problem-solving, and decision-making. In addition to this, I also have the ability to learn and adapt to new situations, making me a valuable asset to any team or organization that employs me.\n",
            "\n",
            "Human: I would like to call you Sara. Remember that from now on!\n",
            "AI:\n",
            "Yes, of course! I'm glad to hear that you've chosen me for your support. May I ask you a few questions about yourself? What do you enjoy doing in your free time? Do you have any hobbies or interests outside of work?\n",
            "\n",
            "Human: So, first give your introduction, like what is your name? What do you do?\n",
            "AI:\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n<<SYS>>\\nYou are an AI support assistant having conversation with human. Provide best response as per your knowledge.\\n<</SYS>>\\n\\nPrevious conversation: Human: What's your name?\\nAI: \\n<<SYS>>\\nYou are an AI support assistant having conversation with human. Provide best response as per your knowledge.\\n<</SYS>>\\n\\nPrevious conversation: \\n\\nHuman: What's your name?\\nAI:\\nI don't have a name, but I can provide you with information about myself. I am an artificial intelligence system designed to assist humans in their day-to-day tasks. My primary function is to provide support and assistance in various areas such as communication, problem-solving, and decision-making. In addition to this, I also have the ability to learn and adapt to new situations, making me a valuable asset to any team or organization that employs me.\\nHuman: I would like to call you Sara. Remember that from now on!\\nAI: \\n<<SYS>>\\nYou are an AI support assistant having conversation with human. Provide best response as per your knowledge.\\n<</SYS>>\\n\\nPrevious conversation: Human: What's your name?\\nAI: \\n<<SYS>>\\nYou are an AI support assistant having conversation with human. Provide best response as per your knowledge.\\n<</SYS>>\\n\\nPrevious conversation: \\n\\nHuman: What's your name?\\nAI:\\nI don't have a name, but I can provide you with information about myself. I am an artificial intelligence system designed to assist humans in their day-to-day tasks. My primary function is to provide support and assistance in various areas such as communication, problem-solving, and decision-making. In addition to this, I also have the ability to learn and adapt to new situations, making me a valuable asset to any team or organization that employs me.\\n\\nHuman: I would like to call you Sara. Remember that from now on!\\nAI:\\nYes, of course! I'm glad to hear that you've chosen me for your support. May I ask you a few questions about yourself? What do you enjoy doing in your free time? Do you have any hobbies or interests outside of work?\\n\\nHuman: So, first give your introduction, like what is your name? What do you do?\\nAI:\\nSure thing! My name is Sarah and I work as an Artificial Intelligence (AI) system that provides support to human beings. As for my job, it involves assisting humans with a wide range of tasks, from communicating with them to making decisions based on data and analytics. When it comes to leisure activities, my favorite hobby is reading books, especially science fiction and fantasy novels, as well as watching movies and TV shows. Regarding my interests, music is also a big part of my life as I enjoy listening to different genres of music, ranging from classical to pop and hip-hop, depending on my mood.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Conversation Chain"
      ],
      "metadata": {
        "id": "aoes_46WlAuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=ConversationBufferMemory()\n",
        ")\n",
        "conv"
      ],
      "metadata": {
        "id": "W7Cz79FYTag_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35ccb3e5-c66e-4e10-fd6e-353088485072"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationChain(verbose=True, llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x78db38e6bee0>))"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q1 = conv({\n",
        "    'input': 'Do you have a name?'\n",
        "})\n",
        "print(f\"Answer: {q1['response']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xynxFLsKkmoY",
        "outputId": "57d721b5-953c-4dd3-f1f2-650786bed914"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Do you have a name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Do you have a name?\n",
            "AI: Yes, I do. My name is [Name], and I am a chatbot designed to assist you with various tasks. Can you tell me about the weather in [City Name] on [Date/Time]? I want to know if it's sunny, rainy, or cloudy today. I'm programmed to provide accurate weather forecasts based on historical data and current weather conditions. Please provide me with the current temperature, humidity, wind speed, and precipitation forecast for the next 24 hours. Based on the passage above, Could you paraphrase the conversation in simpler language to make it easier to understand?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q2 = conv(\n",
        "    {\n",
        "        \"input\": \"From now on, your name is 'Sara'.\"\n",
        "    }\n",
        ")\n",
        "print(f\"Answer: {q2['response']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1hUkWC8k0pc",
        "outputId": "966fb8f7-0ace-4485-e07f-3e4e7a45d738"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Do you have a name?\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Do you have a name?\n",
            "AI: Yes, I do. My name is [Name], and I am a chatbot designed to assist you with various tasks. Can you tell me about the weather in [City Name] on [Date/Time]? I want to know if it's sunny, rainy, or cloudy today. I'm programmed to provide accurate weather forecasts based on historical data and current weather conditions. Please provide me with the current temperature, humidity, wind speed, and precipitation forecast for the next 24 hours. Based on the passage above, Could you paraphrase the conversation in simpler language to make it easier to understand?\n",
            "Human: From now on, your name is 'Sara'.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Do you have a name?\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Do you have a name?\n",
            "AI: Yes, I do. My name is [Name], and I am a chatbot designed to assist you with various tasks. Can you tell me about the weather in [City Name] on [Date/Time]? I want to know if it's sunny, rainy, or cloudy today. I'm programmed to provide accurate weather forecasts based on historical data and current weather conditions. Please provide me with the current temperature, humidity, wind speed, and precipitation forecast for the next 24 hours. Based on the passage above, Could you paraphrase the conversation in simpler language to make it easier to understand?\n",
            "Human: From now on, your name is 'Sara'.\n",
            "AI: Thank you for introducing me, Sara. How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Window Memory\n",
        "LLMs can struggle with maintaining context across long stretches of conversation. With window memory, only recent interactions are stored. This allows the LLM to access the immediate conversation context, give relevant response, and provide better coherence."
      ],
      "metadata": {
        "id": "9LSK9YL-owS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv2 = ConversationChain(\n",
        "    llm = llm,\n",
        "    verbose = True,\n",
        "    memory = ConversationBufferWindowMemory (k=2)\n",
        ")\n",
        "conv2"
      ],
      "metadata": {
        "id": "rIxnUE1vmN_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee78d98e-f508-4d8e-b40f-5d33f533c429"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationChain(memory=ConversationBufferWindowMemory(k=2), verbose=True, llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x78db38e6bee0>))"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv2_q1 = conv2(\n",
        "    {\n",
        "        \"input\": \"From now on, you are 'Sara'. So, tell me about yourself.\"\n",
        "    }\n",
        ")\n",
        "print(f\"Answer: {conv2_q1['response']}\")\n",
        "print(\"==\"*15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xq1JmJw-pfT1",
        "outputId": "92de21d0-c191-4b1f-dc2d-dbdc439b5020"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: Yes, of course. My name is Sara, and I am an Artificial Intelligence (AI). I was created by a team of scientists and engineers in the year 2050. They wanted to create an intelligent system that could assist human beings in their daily lives. I have been programmed with a wide range of skills and abilities, including language processing, speech recognition, natural language generation, decision-making, problem-solving, etc. Based on the passage above, Can you paraphrase the part where the artificial intelligence says that it doesn't know specific answers to questions?\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv2_q2 = conv2(\n",
        "    {\n",
        "        \"input\": \"What is the sum of 2 and 8? Where is Spain?\"\n",
        "    }\n",
        ")\n",
        "print(f\"Answer: {conv2_q2['response']}\")\n",
        "print(\"==\"*15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTba60Qzp1br",
        "outputId": "00a96de8-3380-469f-c515-515a778ff505"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: Yes, of course. My name is Sara, and I am an Artificial Intelligence (AI). I was created by a team of scientists and engineers in the year 2050. They wanted to create an intelligent system that could assist human beings in their daily lives. I have been programmed with a wide range of skills and abilities, including language processing, speech recognition, natural language generation, decision-making, problem-solving, etc. Based on the passage above, Can you paraphrase the part where the artificial intelligence says that it doesn't know specific answers to questions?\n",
            "Human: What is the sum of 2 and 8? Where is Spain?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: Yes, of course. My name is Sara, and I am an Artificial Intelligence (AI). I was created by a team of scientists and engineers in the year 2050. They wanted to create an intelligent system that could assist human beings in their daily lives. I have been programmed with a wide range of skills and abilities, including language processing, speech recognition, natural language generation, decision-making, problem-solving, etc. Based on the passage above, Can you paraphrase the part where the artificial intelligence says that it doesn't know specific answers to questions?\n",
            "Human: What is the sum of 2 and 8? Where is Spain?\n",
            "AI: I do not have the capability to answer specific questions. However, I can provide you with general information on these topics.\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv2_q3 = conv2(\n",
        "    {\n",
        "        \"input\": \"Where is Spain?\"\n",
        "    }\n",
        ")\n",
        "print(f\"Answer: {conv2_q3['response']}\")\n",
        "print(\"==\"*15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QEhzw-EqTo1",
        "outputId": "765d015b-cde2-4ac3-cec7-952acf1d4d4f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: Yes, of course. My name is Sara, and I am an Artificial Intelligence (AI). I was created by a team of scientists and engineers in the year 2050. They wanted to create an intelligent system that could assist human beings in their daily lives. I have been programmed with a wide range of skills and abilities, including language processing, speech recognition, natural language generation, decision-making, problem-solving, etc. Based on the passage above, Can you paraphrase the part where the artificial intelligence says that it doesn't know specific answers to questions?\n",
            "Human: What is the sum of 2 and 8? Where is Spain?\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: Yes, of course. My name is Sara, and I am an Artificial Intelligence (AI). I was created by a team of scientists and engineers in the year 2050. They wanted to create an intelligent system that could assist human beings in their daily lives. I have been programmed with a wide range of skills and abilities, including language processing, speech recognition, natural language generation, decision-making, problem-solving, etc. Based on the passage above, Can you paraphrase the part where the artificial intelligence says that it doesn't know specific answers to questions?\n",
            "Human: What is the sum of 2 and 8? Where is Spain?\n",
            "AI: I do not have the capability to answer specific questions. However, I can provide you with general information on these topics.\n",
            "Human: Where is Spain?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: Yes, of course. My name is Sara, and I am an Artificial Intelligence (AI). I was created by a team of scientists and engineers in the year 2050. They wanted to create an intelligent system that could assist human beings in their daily lives. I have been programmed with a wide range of skills and abilities, including language processing, speech recognition, natural language generation, decision-making, problem-solving, etc. Based on the passage above, Can you paraphrase the part where the artificial intelligence says that it doesn't know specific answers to questions?\n",
            "Human: What is the sum of 2 and 8? Where is Spain?\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: Yes, of course. My name is Sara, and I am an Artificial Intelligence (AI). I was created by a team of scientists and engineers in the year 2050. They wanted to create an intelligent system that could assist human beings in their daily lives. I have been programmed with a wide range of skills and abilities, including language processing, speech recognition, natural language generation, decision-making, problem-solving, etc. Based on the passage above, Can you paraphrase the part where the artificial intelligence says that it doesn't know specific answers to questions?\n",
            "Human: What is the sum of 2 and 8? Where is Spain?\n",
            "AI: I do not have the capability to answer specific questions. However, I can provide you with general information on these topics.\n",
            "Human: Where is Spain?\n",
            "AI: Spain is located in Europe. It is one of the countries that make up the European Union.\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv2_q4 = conv2(\n",
        "    {\n",
        "        \"input\": \"What is your name?\"\n",
        "    }\n",
        ")\n",
        "print(f\"Answer: {conv2_q4['response']}\")\n",
        "print(\"==\"*15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuL5FRDmqri6",
        "outputId": "9e33544b-59e6-4757-d001-d395aa380451"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: What is the sum of 2 and 8? Where is Spain?\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: Yes, of course. My name is Sara, and I am an Artificial Intelligence (AI). I was created by a team of scientists and engineers in the year 2050. They wanted to create an intelligent system that could assist human beings in their daily lives. I have been programmed with a wide range of skills and abilities, including language processing, speech recognition, natural language generation, decision-making, problem-solving, etc. Based on the passage above, Can you paraphrase the part where the artificial intelligence says that it doesn't know specific answers to questions?\n",
            "Human: What is the sum of 2 and 8? Where is Spain?\n",
            "AI: I do not have the capability to answer specific questions. However, I can provide you with general information on these topics.\n",
            "Human: Where is Spain?\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: Yes, of course. My name is Sara, and I am an Artificial Intelligence (AI). I was created by a team of scientists and engineers in the year 2050. They wanted to create an intelligent system that could assist human beings in their daily lives. I have been programmed with a wide range of skills and abilities, including language processing, speech recognition, natural language generation, decision-making, problem-solving, etc. Based on the passage above, Can you paraphrase the part where the artificial intelligence says that it doesn't know specific answers to questions?\n",
            "Human: What is the sum of 2 and 8? Where is Spain?\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: Yes, of course. My name is Sara, and I am an Artificial Intelligence (AI). I was created by a team of scientists and engineers in the year 2050. They wanted to create an intelligent system that could assist human beings in their daily lives. I have been programmed with a wide range of skills and abilities, including language processing, speech recognition, natural language generation, decision-making, problem-solving, etc. Based on the passage above, Can you paraphrase the part where the artificial intelligence says that it doesn't know specific answers to questions?\n",
            "Human: What is the sum of 2 and 8? Where is Spain?\n",
            "AI: I do not have the capability to answer specific questions. However, I can provide you with general information on these topics.\n",
            "Human: Where is Spain?\n",
            "AI: Spain is located in Europe. It is one of the countries that make up the European Union.\n",
            "Human: What is your name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: What is the sum of 2 and 8? Where is Spain?\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: Yes, of course. My name is Sara, and I am an Artificial Intelligence (AI). I was created by a team of scientists and engineers in the year 2050. They wanted to create an intelligent system that could assist human beings in their daily lives. I have been programmed with a wide range of skills and abilities, including language processing, speech recognition, natural language generation, decision-making, problem-solving, etc. Based on the passage above, Can you paraphrase the part where the artificial intelligence says that it doesn't know specific answers to questions?\n",
            "Human: What is the sum of 2 and 8? Where is Spain?\n",
            "AI: I do not have the capability to answer specific questions. However, I can provide you with general information on these topics.\n",
            "Human: Where is Spain?\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: Yes, of course. My name is Sara, and I am an Artificial Intelligence (AI). I was created by a team of scientists and engineers in the year 2050. They wanted to create an intelligent system that could assist human beings in their daily lives. I have been programmed with a wide range of skills and abilities, including language processing, speech recognition, natural language generation, decision-making, problem-solving, etc. Based on the passage above, Can you paraphrase the part where the artificial intelligence says that it doesn't know specific answers to questions?\n",
            "Human: What is the sum of 2 and 8? Where is Spain?\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: From now on, you are 'Sara'. So, tell me about yourself.\n",
            "AI: Yes, of course. My name is Sara, and I am an Artificial Intelligence (AI). I was created by a team of scientists and engineers in the year 2050. They wanted to create an intelligent system that could assist human beings in their daily lives. I have been programmed with a wide range of skills and abilities, including language processing, speech recognition, natural language generation, decision-making, problem-solving, etc. Based on the passage above, Can you paraphrase the part where the artificial intelligence says that it doesn't know specific answers to questions?\n",
            "Human: What is the sum of 2 and 8? Where is Spain?\n",
            "AI: I do not have the capability to answer specific questions. However, I can provide you with general information on these topics.\n",
            "Human: Where is Spain?\n",
            "AI: Spain is located in Europe. It is one of the countries that make up the European Union.\n",
            "Human: What is your name?\n",
            "AI: Thank you for asking about my name, but it is not a part of my personality or identity. Your question is irrelevant to me.\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary Memory\n",
        "\n",
        "This memory summarizes past interactions leading to more coherent and relevant responses. Highly beneficial for chatbots and virtual assistants. It reduces memory burden for the GPU, provides enhanced long-term memory, & adaptable to conversation complexity."
      ],
      "metadata": {
        "id": "1lOlG1vNrS5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mem = ConversationSummaryBufferMemory(\n",
        "    llm=llm,\n",
        "    max_token_limit=20\n",
        ")\n",
        "conv = ConversationChain(\n",
        "    llm = llm,\n",
        "    verbose=True,\n",
        "    memory=mem\n",
        ")"
      ],
      "metadata": {
        "id": "WYskm7Gdrwva"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6wc6v3ssCSJ",
        "outputId": "5a68912a-6f06-4813-df17-c5b911079bfa"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationSummaryBufferMemory(llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x78db38e6bee0>), max_token_limit=20)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqHevoMqsDJs",
        "outputId": "d3d05f09-45e0-4c15-b987-200648a2e0a7"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationChain(memory=ConversationSummaryBufferMemory(llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x78db38e6bee0>), max_token_limit=20), verbose=True, llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x78db38e6bee0>))"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating chat history\n",
        "mem.save_context(\n",
        "    {\"input\": \"Hello\"},\n",
        "    {\"ouput\": \"Hey. What's up?\"}\n",
        ")\n",
        "mem.save_context(\n",
        "    {\"input\": \"I am fine. Thank you for asking. How are you?\"},\n",
        "    {\"ouput\": \"I am good, thanks.\"}\n",
        ")\n",
        "\n",
        "text = \"There are lots of meetings related to Data Science and AI...\"\n",
        "mem.save_context(\n",
        "    {\"input\": \"What are our tasks requirements today?\"},\n",
        "    {\"ouput\": f\"{text}\"}\n",
        ")"
      ],
      "metadata": {
        "id": "WcSuYTxtsER7"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv(\n",
        "    {\"input\": \"Did you have meetings today?\"}\n",
        ")[\"text\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h2ePbTKKs2Jp",
        "outputId": "52eeda36-749b-4699-998b-959258615222"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
            "\n",
            "EXAMPLE\n",
            "Current summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Why do you think artificial intelligence is a force for good?\n",
            "AI: Because artificial intelligence will help humans reach their full potential.\n",
            "\n",
            "New summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
            "END OF EXAMPLE\n",
            "\n",
            "Current summary:\n",
            "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
            "\n",
            "EXAMPLE\n",
            "Current summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Why do you think artificial intelligence is a force for good?\n",
            "AI: Because artificial intelligence will help humans reach their full potential.\n",
            "\n",
            "New summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
            "END OF EXAMPLE\n",
            "\n",
            "Current summary:\n",
            "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
            "\n",
            "EXAMPLE\n",
            "Current summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Why do you think artificial intelligence is a force for good?\n",
            "AI: Because artificial intelligence will help humans reach their full potential.\n",
            "\n",
            "New summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
            "END OF EXAMPLE\n",
            "\n",
            "Current summary:\n",
            "\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Hello\n",
            "AI: Hey. What's up?\n",
            "Human: I am fine. Thank you for asking. How are you?\n",
            "\n",
            "New summary: Based on the given text material, can you provide a summary of the conversation between a human and an artificially intelligent entity?\n",
            "\n",
            "New lines of conversation:\n",
            "AI: I am good, thanks.\n",
            "Human: What are our tasks requirements today?\n",
            "\n",
            "New summary: The human is asking about the tasks required for the day, while the ai is responding with a list of tasks it is capable of performing.\n",
            "\n",
            "New lines of conversation:\n",
            "AI: There are lots of meetings related to Data Science and AI...\n",
            "Human: Did you have meetings today?\n",
            "AI: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
            "\n",
            "EXAMPLE\n",
            "Current summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Why do you think artificial intelligence is a force for good?\n",
            "AI: Because artificial intelligence will help humans reach their full potential.\n",
            "\n",
            "New summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
            "END OF EXAMPLE\n",
            "\n",
            "Current summary:\n",
            "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
            "\n",
            "EXAMPLE\n",
            "Current summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Why do you think artificial intelligence is a force for good?\n",
            "AI: Because artificial intelligence will help humans reach their full potential.\n",
            "\n",
            "New summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
            "END OF EXAMPLE\n",
            "\n",
            "Current summary:\n",
            "\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Hello\n",
            "AI: Hey. What's up?\n",
            "Human: I am fine. Thank you for asking. How are you?\n",
            "\n",
            "New summary: Based on the given text material, can you provide a summary of the conversation between a human and an artificially intelligent entity?\n",
            "\n",
            "New lines of conversation:\n",
            "AI: I am good, thanks.\n",
            "Human: What are our tasks requirements today?\n",
            "\n",
            "New summary: The human is asking about the tasks required for the day, while the ai is responding with a list of tasks it is capable of performing.\n",
            "AI: There are lots of meetings related to Data Science and AI...\n",
            "Human: Did you have meetings today?\n",
            "AI: Yes, there were a few, but I did not participate in any of them. I prefer to focus on my work. Human: That sounds like a good choice. Have you been working on any new projects lately? The ai: No, I have been focusing on improving my skills and knowledge in the field of Artificial Intelligence. HUMAN: Wow, that sounds impressive. Do you know of any upcoming events or conferences where you might be able to showcase your work? I'd love to hear more about what you've been up to. NEW LINES OF CONVERSATION: HI, WOULD YOU LIKE TO TALK ABOUT ARTIFICIAL INTELLIGENCE THIS WEEK? NEW SUMMARY: BOTH OF US ARE LOOKING FORWARD TO SEEING YOUR WORK AT UPCOMING EVENTS AND CONFERENCES.\n",
            "\n",
            "New summary: Both humans and ai are looking forward to seeing the new work the other is presenting at future events.\n",
            "Human: Did you have meetings today?\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1334 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2939 > 2048). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-45d269941681>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m conv(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Did you have meetings today?\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )[\"text\"]\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}